{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def str_to_list(s):\n",
    "    return [float(x.strip()) for x in s.strip('[]').split(',')]\n",
    "\n",
    "# Custom converters for array columns\n",
    "array_converters = {'x': str_to_list, 'y': str_to_list, 'z': str_to_list}\n",
    "\n",
    "# Read training data\n",
    "training_data = pd.read_csv(\"./data/gesture_data.csv\", \n",
    "                            converters=array_converters)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"Training data 'x' column first row:\")\n",
    "print(training_data.loc[0, 'x'])\n",
    "print(\"\\nType of 'x' column first row:\")\n",
    "print(type(training_data.loc[0, 'x']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking how many data points per axes\n",
    "mydict = {'x': [9999,0],\n",
    "          'y': [9999,0],\n",
    "          'z': [9999,0]}\n",
    "for i in range(len(training_data)):\n",
    "    for axes, hidden in mydict.items():\n",
    "        if len(training_data.loc[i, axes]) < hidden[0]:\n",
    "            mydict[axes][0] = len(training_data.loc[i, axes])\n",
    "        if len(training_data.loc[i, axes]) > hidden[1]:\n",
    "            mydict[axes][1] = len(training_data.loc[i, axes])\n",
    "\n",
    "print(mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pad or truncate the array\n",
    "def pad_or_truncate(array, target_length=40):\n",
    "    if len(array) > target_length:\n",
    "        return array[:target_length]\n",
    "    elif len(array) < target_length:\n",
    "        return array + [0] * (target_length - len(array))\n",
    "    else:\n",
    "        return array\n",
    "\n",
    "for col in [\"x\",\"y\",\"z\"]:\n",
    "    training_data[col] = training_data[col].apply(pad_or_truncate)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into 50% Train, 25% Test, and 25% hiddenidation\n",
    "train_data, temp_data = train_test_split(training_data, test_size=0.5, random_state=42)\n",
    "test_data, hidden_data = train_test_split(temp_data, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy.interpolate import CubicSpline\n",
    "# Function for data augmentation using CubicSpline\n",
    "def time_warping(time_series, num_operations, warp_factor):\n",
    "    \"\"\"\n",
    "    Applying time warping to a time series with balanced insertions and deletions.\n",
    "\n",
    "    :param time_series: Time series, numpy array.\n",
    "    :param num_operations: Number of operations (half will be insertions and half deletions).\n",
    "    :param warp_factor: Warp factor that determines the impact of operations.\n",
    "    :return: Distorted time series with the same length.\n",
    "    \"\"\"\n",
    "    warped_series = time_series.copy()\n",
    "    \n",
    "    # Ensure the number of insertions equals the number of deletions\n",
    "    num_insertions = num_operations // 2\n",
    "    num_deletions = num_insertions\n",
    "    \n",
    "    for _ in range(num_insertions):\n",
    "        index = random.randint(1, len(warped_series) - 2)\n",
    "        insertion_hiddenue = (warped_series[index - 1] + warped_series[index]) * 0.5\n",
    "        warp_amount = insertion_hiddenue * warp_factor * random.uniform(-1, 1)\n",
    "        warped_series = np.insert(warped_series, index, insertion_hiddenue + warp_amount)\n",
    "    \n",
    "    for _ in range(num_deletions):\n",
    "        if len(warped_series) > 2:  # Ensure there's enough data to delete from\n",
    "            index = random.randint(1, len(warped_series) - 2)\n",
    "            warped_series = np.delete(warped_series, index)\n",
    "    \n",
    "    return warped_series\n",
    "\n",
    "def magnitude_warping(time_series, num_knots, warp_std_dev):\n",
    "    \"\"\"\n",
    "    Applies magnitude warping to a time series using cubic splines.\n",
    "\n",
    "    :param time_series: np.array, time series to distort\n",
    "    :param num_knots: int, number of control points for splines\n",
    "    :param warp_std_dev: float, standard deviation for distorting the hiddenues of control points\n",
    "    :return: np.array, distorted time series\n",
    "    \"\"\"\n",
    "    # Generating random spline knots within a time series\n",
    "    knot_positions = np.linspace(0, len(time_series) - 1, num=num_knots)\n",
    "    knot_hiddenues = 1 + np.random.normal(0, warp_std_dev, num_knots)\n",
    "\n",
    "    # Creating a Cubic Spline Function Through Knots\n",
    "    spline = CubicSpline(knot_positions, knot_hiddenues)\n",
    "\n",
    "    # Generating time indexes for a time series\n",
    "    time_indexes = np.arange(len(time_series))\n",
    "\n",
    "    # Applying distortion to a time series\n",
    "    warped_time_series = time_series * spline(time_indexes)\n",
    "\n",
    "    return warped_time_series\n",
    "\n",
    "def augment_data(x, y, z, num_augmented=1, time_warp_factor=0.05, mag_warp_std_dev=0.05):\n",
    "    \"\"\"\n",
    "    Augments vibrational data by applying time warping and magnitude warping.\n",
    "\n",
    "    :param x: np.array, x-axis vibrational data\n",
    "    :param y: np.array, y-axis vibrational data\n",
    "    :param z: np.array, z-axis vibrational data\n",
    "    :param num_augmented: int, number of augmented samples to generate\n",
    "    :param num_operations: int, number of operations for time warping\n",
    "    :param time_warp_factor: float, factor determining the magnitude of time warping\n",
    "    :param mag_warp_knots: int, number of control points for magnitude warping splines\n",
    "    :param mag_warp_std_dev: float, standard deviation for magnitude warping\n",
    "    :return: list of dictionaries containing augmented x, y, and z data\n",
    "    \"\"\"\n",
    "    original_length = len(x)\n",
    "    t = np.arange(original_length)\n",
    "    \n",
    "    \n",
    "    # Define a midpoint range factor\n",
    "    midpoint_factor = 0.5\n",
    "    deviation_factor = 0.2\n",
    "\n",
    "    # Calculate the midpoint and range for operations and knots\n",
    "    midpoint = int(original_length * midpoint_factor)\n",
    "    min_hidden = int(midpoint * (1 - deviation_factor))\n",
    "    max_hidden = int(midpoint * (1 + deviation_factor))\n",
    "\n",
    "    # Generate random hiddenues within this controlled range\n",
    "    num_operations = random.randint(min_hidden, max_hidden)\n",
    "    mag_warp_knots = random.randint(min_hidden, max_hidden)\n",
    "    \n",
    "    augmented_data = []\n",
    "    for _ in range(num_augmented):\n",
    "        # Apply time warping to each axis\n",
    "        warped_x = time_warping(x, num_operations=num_operations, warp_factor=time_warp_factor)\n",
    "        warped_y = time_warping(y, num_operations=num_operations, warp_factor=time_warp_factor)\n",
    "        warped_z = time_warping(z, num_operations=num_operations, warp_factor=time_warp_factor)\n",
    "        \n",
    "        # Apply magnitude warping to each axis\n",
    "        warped_x = magnitude_warping(warped_x, num_knots=mag_warp_knots, warp_std_dev=mag_warp_std_dev)\n",
    "        warped_y = magnitude_warping(warped_y, num_knots=mag_warp_knots, warp_std_dev=mag_warp_std_dev)\n",
    "        warped_z = magnitude_warping(warped_z, num_knots=mag_warp_knots, warp_std_dev=mag_warp_std_dev)\n",
    "        \n",
    "        augmented_data.append({'x': warped_x.tolist(), 'y': warped_y.tolist(), 'z': warped_z.tolist()})\n",
    "    \n",
    "    return augmented_data\n",
    "\n",
    "# Augment training data\n",
    "augmented_data = []\n",
    "for _, row in train_data.iterrows():\n",
    "    augmented = augment_data(row['x'], row['y'], row['z'], num_augmented=4)\n",
    "    for aug in augmented:\n",
    "        new_row = row.copy()\n",
    "        new_row['x'] = aug['x']\n",
    "        new_row['y'] = aug['y']\n",
    "        new_row['z'] = aug['z']\n",
    "        augmented_data.append(new_row)\n",
    "\n",
    "# Combine original and augmented data\n",
    "augmented_train_data = pd.concat([train_data, pd.DataFrame(augmented_data)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "# Find min and max hiddenues from the augmented training data\n",
    "min_hiddenues = {}\n",
    "max_hiddenues = {}\n",
    "\n",
    "for column in ['x', 'y', 'z']:\n",
    "    min_hiddenues[column] = np.min([np.min(row) for row in augmented_train_data[column] if isinstance(row, list)])\n",
    "    max_hiddenues[column] = np.max([np.max(row) for row in augmented_train_data[column] if isinstance(row, list)])\n",
    "\n",
    "# Normalization function\n",
    "def normalize_array(arr, min_hidden, max_hidden):\n",
    "    return [2 * ((x - min_hidden) / (max_hidden - min_hidden)) - 1 for x in arr]\n",
    "\n",
    "for col in ['x', 'y', 'z']:\n",
    "    augmented_train_data[col] = augmented_train_data[col].apply(normalize_array, args=(min_hiddenues[col], max_hiddenues[col]))\n",
    "    test_data[col] = test_data[col].apply(normalize_array, args=(min_hiddenues[col], max_hiddenues[col]))\n",
    "    hidden_data[col] = hidden_data[col].apply(normalize_array, args=(min_hiddenues[col], max_hiddenues[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for min and max lengths in the data\n",
    "min_max_lengths = {'x': [9999, 0], 'y': [9999, 0], 'z': [9999, 0]}\n",
    "for i in range(len(augmented_train_data)):\n",
    "    for axes, hidden in min_max_lengths.items():\n",
    "        if len(augmented_train_data.loc[i, axes]) < hidden[0]:\n",
    "            min_max_lengths[axes][0] = len(augmented_train_data.loc[i, axes])\n",
    "        if len(augmented_train_data.loc[i, axes]) > hidden[1]:\n",
    "            min_max_lengths[axes][1] = len(augmented_train_data.loc[i, axes])\n",
    "\n",
    "print(min_max_lengths)\n",
    "\n",
    "# Find final min and max hiddenues\n",
    "min_hiddenues = {}\n",
    "max_hiddenues = {}\n",
    "\n",
    "for column in ['x', 'y', 'z']:\n",
    "    min_hiddenues[column] = np.min([np.min(row) for row in augmented_train_data[column] if isinstance(row, list)])\n",
    "    max_hiddenues[column] = np.max([np.max(row) for row in augmented_train_data[column] if isinstance(row, list)])\n",
    "\n",
    "for column in ['x', 'y', 'z']:\n",
    "    print(f\"{column} - Min: {min_hiddenues[column]}, Max: {max_hiddenues[column]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print head for augmented data\n",
    "print(augmented_train_data.head())\n",
    "print(test_data.head())\n",
    "print(hidden_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert NumPy arrays to lists of floats for all the relevant columns\n",
    "augmented_train_data['x'] = augmented_train_data['x'].apply(lambda x: list(map(float, x)))\n",
    "augmented_train_data['y'] = augmented_train_data['y'].apply(lambda y: list(map(float, y)))\n",
    "augmented_train_data['z'] = augmented_train_data['z'].apply(lambda z: list(map(float, z)))\n",
    "\n",
    "test_data['x'] = test_data['x'].apply(lambda x: list(map(float, x)))\n",
    "test_data['y'] = test_data['y'].apply(lambda y: list(map(float, y)))\n",
    "test_data['z'] = test_data['z'].apply(lambda z: list(map(float, z)))\n",
    "\n",
    "hidden_data['x'] = hidden_data['x'].apply(lambda x: list(map(float, x)))\n",
    "hidden_data['y'] = hidden_data['y'].apply(lambda y: list(map(float, y)))\n",
    "hidden_data['z'] = hidden_data['z'].apply(lambda z: list(map(float, z)))\n",
    "\n",
    "augmented_train_data.to_csv(\"./processed/processed_train_gesture_data.csv\", index=False)\n",
    "test_data.to_csv(\"./processed/processed_test_gesture_data.csv\", index=False)\n",
    "hidden_data.to_csv(\"./processed/processed_hidden_gesture_data.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
